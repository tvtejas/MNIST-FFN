{"cells":[{"metadata":{"_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \n\nfrom pathlib import Path\nimport torch\nfrom torch.utils.data import TensorDataset ,DataLoader\nfrom torch import nn,optim\nimport torch.nn.functional as F\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nfrom sklearn.model_selection import train_test_split\n\n# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.","execution_count":50,"outputs":[{"output_type":"stream","text":"/kaggle/input/test.csv\n/kaggle/input/train.csv\n/kaggle/input/sample_submission.csv\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"train=pd.read_csv('/kaggle/input/train.csv')\ntest=pd.read_csv('/kaggle/input/test.csv')\ntrain.shape,test.shape","execution_count":51,"outputs":[{"output_type":"execute_result","execution_count":51,"data":{"text/plain":"((42000, 785), (28000, 784))"},"metadata":{}}]},{"metadata":{"trusted":true},"cell_type":"code","source":"x=train.drop(\"label\",axis=1)\ny=np.array(train['label'])\nx.shape,y.shape","execution_count":52,"outputs":[{"output_type":"execute_result","execution_count":52,"data":{"text/plain":"((42000, 784), (42000,))"},"metadata":{}}]},{"metadata":{"trusted":true},"cell_type":"code","source":"torch_X_train = torch.from_numpy(x.values).type(torch.FloatTensor)/255\ntorch_y_train = torch.from_numpy(y).type(torch.LongTensor)\nmyDataset = torch.utils.data.TensorDataset(torch_X_train,torch_y_train)\nvalid_no  = int(0.2 * len(myDataset))\n# so divide the data into trainset and testset\ntrainSet,testSet = torch.utils.data.random_split(myDataset,(len(myDataset)-valid_no,valid_no))\nprint(f\"len of trainSet {len(trainSet)} , len of testSet {len(testSet)}\")\nbatch_size=64\ntrain_loader  = DataLoader(trainSet , batch_size=batch_size ,shuffle=True) \ntest_loader  = DataLoader(testSet , batch_size=batch_size ,shuffle=True)","execution_count":53,"outputs":[{"output_type":"stream","text":"len of trainSet 33600 , len of testSet 8400\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"from torch import nn, optim\nclass Network(nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.fc1 = nn.Linear(784, 256)\n        self.fc2 = nn.Linear(256, 128)\n        self.fc3 = nn.Linear(128, 64)\n        self.fc4 = nn.Linear(64, 10)\n\n        # Dropout module with 0.2 drop probability\n        self.dropout = nn.Dropout(p=0.2)\n\n    def forward(self, x):\n        # make sure input tensor is flattened\n        x = x.view(x.shape[0], -1)\n\n        # Now with dropout\n        x = self.dropout(F.relu(self.fc1(x)))\n        x = self.dropout(F.relu(self.fc2(x)))\n        x = self.dropout(F.relu(self.fc3(x)))\n\n        # output so no dropout here\n        x = F.log_softmax(self.fc4(x), dim=1)\n\n        return x\n        \nmodel=Network()\noptimizer=torch.optim.SGD(model.parameters(), lr=0.01, weight_decay= 1e-6, momentum = 0.9,nesterov = True)\ncriterion=nn.CrossEntropyLoss()","execution_count":54,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"epochs=30\ntrain_losses,test_losses=[],[]\nfor e in range(epochs):\n    running_loss=0\n    for images,labels in train_loader:\n        optimizer.zero_grad()\n        log_ps=model(images)\n        loss=criterion(log_ps,labels)\n        loss.backward()\n        optimizer.step()\n        running_loss+=loss.item()\n        \n    else:\n        test_loss=0\n        accuracy=0\n        \n        with torch.no_grad():\n            model.eval()\n            for images,labels in test_loader:\n                log_ps=model(images)\n                test_loss+=criterion(log_ps,labels)\n                ps=torch.exp(log_ps)\n                top_p,top_class=ps.topk(1,dim=1)\n                equals=top_class==labels.view(*top_class.shape)\n                accuracy+=torch.mean(equals.type(torch.FloatTensor))\n        model.train()\n        train_losses.append(running_loss/len(train_loader))\n        test_losses.append(test_loss/len(test_loader))\n\n        print(\"Epoch: {}/{}.. \".format(e+1, epochs),\n              \"Training Loss: {:.3f}.. \".format(running_loss/len(train_loader)),\n              \"Test Loss: {:.3f}.. \".format(test_loss/len(test_loader)),\n              \"Test Accuracy: {:.3f}\".format(accuracy/len(test_loader)))   ","execution_count":55,"outputs":[{"output_type":"stream","text":"Epoch: 1/30..  Training Loss: 1.196..  Test Loss: 0.353..  Test Accuracy: 0.897\nEpoch: 2/30..  Training Loss: 0.352..  Test Loss: 0.205..  Test Accuracy: 0.938\nEpoch: 3/30..  Training Loss: 0.238..  Test Loss: 0.157..  Test Accuracy: 0.950\nEpoch: 4/30..  Training Loss: 0.183..  Test Loss: 0.127..  Test Accuracy: 0.961\nEpoch: 5/30..  Training Loss: 0.149..  Test Loss: 0.108..  Test Accuracy: 0.965\nEpoch: 6/30..  Training Loss: 0.127..  Test Loss: 0.105..  Test Accuracy: 0.965\nEpoch: 7/30..  Training Loss: 0.112..  Test Loss: 0.094..  Test Accuracy: 0.970\nEpoch: 8/30..  Training Loss: 0.098..  Test Loss: 0.092..  Test Accuracy: 0.971\nEpoch: 9/30..  Training Loss: 0.087..  Test Loss: 0.089..  Test Accuracy: 0.972\nEpoch: 10/30..  Training Loss: 0.079..  Test Loss: 0.083..  Test Accuracy: 0.975\nEpoch: 11/30..  Training Loss: 0.070..  Test Loss: 0.082..  Test Accuracy: 0.974\nEpoch: 12/30..  Training Loss: 0.067..  Test Loss: 0.079..  Test Accuracy: 0.975\nEpoch: 13/30..  Training Loss: 0.057..  Test Loss: 0.075..  Test Accuracy: 0.977\nEpoch: 14/30..  Training Loss: 0.054..  Test Loss: 0.071..  Test Accuracy: 0.978\nEpoch: 15/30..  Training Loss: 0.051..  Test Loss: 0.078..  Test Accuracy: 0.978\nEpoch: 16/30..  Training Loss: 0.042..  Test Loss: 0.073..  Test Accuracy: 0.977\nEpoch: 17/30..  Training Loss: 0.042..  Test Loss: 0.085..  Test Accuracy: 0.975\nEpoch: 18/30..  Training Loss: 0.038..  Test Loss: 0.075..  Test Accuracy: 0.978\nEpoch: 19/30..  Training Loss: 0.036..  Test Loss: 0.077..  Test Accuracy: 0.978\nEpoch: 20/30..  Training Loss: 0.036..  Test Loss: 0.071..  Test Accuracy: 0.980\nEpoch: 21/30..  Training Loss: 0.033..  Test Loss: 0.084..  Test Accuracy: 0.976\nEpoch: 22/30..  Training Loss: 0.030..  Test Loss: 0.084..  Test Accuracy: 0.977\nEpoch: 23/30..  Training Loss: 0.030..  Test Loss: 0.080..  Test Accuracy: 0.978\nEpoch: 24/30..  Training Loss: 0.031..  Test Loss: 0.076..  Test Accuracy: 0.978\nEpoch: 25/30..  Training Loss: 0.024..  Test Loss: 0.089..  Test Accuracy: 0.976\nEpoch: 26/30..  Training Loss: 0.024..  Test Loss: 0.075..  Test Accuracy: 0.980\nEpoch: 27/30..  Training Loss: 0.026..  Test Loss: 0.080..  Test Accuracy: 0.980\nEpoch: 28/30..  Training Loss: 0.023..  Test Loss: 0.083..  Test Accuracy: 0.979\nEpoch: 29/30..  Training Loss: 0.020..  Test Loss: 0.080..  Test Accuracy: 0.979\nEpoch: 30/30..  Training Loss: 0.020..  Test Loss: 0.086..  Test Accuracy: 0.979\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(\"Our model: \\n\\n\", model, '\\n')\nprint(\"The state dict keys: \\n\\n\", model.state_dict().keys())","execution_count":56,"outputs":[{"output_type":"stream","text":"Our model: \n\n Network(\n  (fc1): Linear(in_features=784, out_features=256, bias=True)\n  (fc2): Linear(in_features=256, out_features=128, bias=True)\n  (fc3): Linear(in_features=128, out_features=64, bias=True)\n  (fc4): Linear(in_features=64, out_features=10, bias=True)\n  (dropout): Dropout(p=0.2)\n) \n\nThe state dict keys: \n\n odict_keys(['fc1.weight', 'fc1.bias', 'fc2.weight', 'fc2.bias', 'fc3.weight', 'fc3.bias', 'fc4.weight', 'fc4.bias'])\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"torch.save(model.state_dict(), 'checkpoint.pth')","execution_count":57,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"state_dict = torch.load('checkpoint.pth')\nprint(state_dict.keys())","execution_count":58,"outputs":[{"output_type":"stream","text":"odict_keys(['fc1.weight', 'fc1.bias', 'fc2.weight', 'fc2.bias', 'fc3.weight', 'fc3.bias', 'fc4.weight', 'fc4.bias'])\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"model.load_state_dict(state_dict)","execution_count":59,"outputs":[{"output_type":"execute_result","execution_count":59,"data":{"text/plain":"IncompatibleKeys(missing_keys=[], unexpected_keys=[])"},"metadata":{}}]},{"metadata":{"trusted":true},"cell_type":"code","source":"\n\ncheckpoint = {'input_size': 784,\n              'output_size': 10,\n              'hidden_layers': [256,128,64],\n              'state_dict': model.state_dict()}\n\ntorch.save(checkpoint, 'checkpoint.pth')\n","execution_count":60,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test_images = pd.read_csv(\"/kaggle/input/test.csv\")\ntest_image = test_images.loc[:,test_images.columns != \"label\"].values\ntest_dataset = torch.from_numpy(test_image).type(torch.FloatTensor)/255\nprint(test_dataset.shape)\n#test_dataset = torch.utils.data.TensorDataset(test_dataset)\nnew_test_loader = torch.utils.data.DataLoader(test_dataset, batch_size = 100, shuffle = False)","execution_count":61,"outputs":[{"output_type":"stream","text":"torch.Size([28000, 784])\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"results = []\nwith torch.no_grad():\n    model.eval()\n    for images in new_test_loader:\n        output = model(images)\n        ps = torch.exp(output)\n        top_p, top_class = ps.topk(1, dim = 1)\n        results += top_class.numpy().tolist()","execution_count":62,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"predictions = np.array(results).flatten()\nprint(predictions[:5])\nprint(predictions.shape)","execution_count":63,"outputs":[{"output_type":"stream","text":"[2 0 9 9 3]\n(28000,)\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"submissions=pd.DataFrame({\"ImageId\": list(range(1,len(predictions)+1)),\n                         \"Label\": predictions})\nsubmissions.to_csv(\"my_submissions.csv\", index=False, header=True)","execution_count":64,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.6.6"}},"nbformat":4,"nbformat_minor":1}